{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 905,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' \\n   Size = 28x28\\n   Representation : 28x28 = \\n   Training set : 60,000\\n   Test set : 10,000       '"
      ]
     },
     "execution_count": 905,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "''' \n",
    "   Size = 28x28\n",
    "   Representation : 28x28 = \n",
    "   Training set : 60,000\n",
    "   Test set : 10,000       '''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 906,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting data/train-images-idx3-ubyte.gz\n",
      "Extracting data/train-labels-idx1-ubyte.gz\n",
      "Extracting data/t10k-images-idx3-ubyte.gz\n",
      "Extracting data/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "#imports\n",
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
    "import ssl\n",
    "ssl._create_default_https_context = ssl._create_unverified_context\n",
    "\n",
    "# Import MNIST data\n",
    "#http://yann.lecun.com/exdb/mnist/\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "mnist = input_data.read_data_sets(\"data/\", one_hot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 907,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Write function to reset session\n",
    "\n",
    "def tf_reset():\n",
    "    try:\n",
    "        sess.close()\n",
    "    except:\n",
    "        pass\n",
    "    tf.reset_default_graph()\n",
    "    return tf.Session()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 908,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Call the reset function\n",
    "\n",
    "sess = tf_reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 909,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define input_placeholder\n",
    "\n",
    "input_ph = tf.placeholder(dtype=tf.float32, shape=[None, 784]) #inputs 28X28=784"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 910,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define output_placeholder\n",
    "\n",
    "n_classes = 10\n",
    "output_ph = tf.placeholder(dtype=tf.float32, shape=[None, n_classes]) #MNIST classes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 911,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Store weigth and biases\n",
    "\n",
    "\n",
    "def get_weights_bias(n_classes): \n",
    "    w = {\n",
    "        \"w1\" : tf.get_variable('W0', shape=(5,5,1,32), initializer=tf.contrib.layers.xavier_initializer()), \n",
    "        \"w2\" : tf.get_variable('W1', shape=(5,5,32,64), initializer=tf.contrib.layers.xavier_initializer()),\n",
    "        'wd1': tf.get_variable('W3', shape=(7*7*64,1024), initializer=tf.contrib.layers.xavier_initializer()), \n",
    "        'out': tf.get_variable('W4', shape=(1024,10), initializer=tf.contrib.layers.xavier_initializer())\n",
    "    }\n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "    b = {\n",
    "        \"b1\" : tf.get_variable('B0', shape=(32), initializer=tf.contrib.layers.xavier_initializer()),\n",
    "        \"b2\" : tf.get_variable('B1', shape=(64), initializer=tf.contrib.layers.xavier_initializer()),\n",
    "        \"b4\" : tf.get_variable('B3', shape=(1024), initializer=tf.contrib.layers.xavier_initializer()),\n",
    "        \"b5\" : tf.get_variable('B4', shape=(10), initializer=tf.contrib.layers.xavier_initializer())\n",
    "    }\n",
    "\n",
    "    return w,b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 912,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Convolve, MaxPool, FullyConnect\n",
    "\n",
    "def conv2d(X,w,b,strides=1):\n",
    "    #building conv-layer\n",
    "    x = tf.nn.conv2d(X,w,strides=[1,strides,strides,1],padding=\"SAME\")\n",
    "    x = tf.nn.bias_add(x,b)\n",
    "    return tf.nn.relu(x)\n",
    "\n",
    "def maxpool2d(X, k=2): #k defines how much you want to reduce your matrix\n",
    "    return tf.nn.max_pool(X, ksize=[1, k, k, 1], strides=[1, k, k, 1],padding='SAME')\n",
    "\n",
    "def fully_connected(X,w,b): #reduce from 2d to 1d\n",
    "    fc = tf.reshape(X, [-1,w.get_shape().as_list()[0]])\n",
    "    fc = tf.matmul(fc, w)\n",
    "    fc = tf.add(fc,b)\n",
    "    return tf.nn.relu(fc)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 913,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Build a model\n",
    "\n",
    "\n",
    "def build_model(input_layer,w,b):\n",
    "\n",
    "    input_layer = tf.reshape(input_layer,[-1,28,28,1])\n",
    "\n",
    "    #conv-layer-1\n",
    "    conv1 = conv2d(input_layer,w['w1'],b['b1'])\n",
    "    conv1 = maxpool2d(conv1, k=2)\n",
    "    print(conv1.get_shape())\n",
    "\n",
    "    conv2 = conv2d(conv1,w['w2'],b['b2'])\n",
    "    conv2 = maxpool2d(conv2, k=2)\n",
    "    print(conv2.get_shape())\n",
    "\n",
    "    fc1 = fully_connected(conv2,w['wd1'],b['b4'])\n",
    "    print(fc1.get_shape())\n",
    "\n",
    "    out = tf.matmul(fc1,w['out'])\n",
    "\n",
    "    return tf.add(out,b['b5'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 914,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(?, 14, 14, 32)\n",
      "(?, 7, 7, 64)\n",
      "(?, 1024)\n"
     ]
    }
   ],
   "source": [
    "#Output_pred = build_model(input_layer,w,b)      \n",
    "\n",
    "w,b = get_weights_bias(10)                                                 \n",
    "output_pred = build_model(input_ph,w,b)          \n",
    "sess.run(tf.global_variables_initializer())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 915,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define Loss function\n",
    "\n",
    "loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=output_pred,labels=output_ph))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 916,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define Optimizer\n",
    "\n",
    "opt = tf.train.AdamOptimizer(learning_rate=0.1).minimize(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 917,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"Softmax:0\", shape=(?, 10), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "#Run session to compute loss\n",
    "\n",
    "pred = tf.nn.softmax(output_pred)\n",
    "sess.run(tf.global_variables_initializer())\n",
    "print (pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 918,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Try for accuracy\n",
    "\n",
    "actual = tf.equal(tf.argmax(pred,1),tf.argmax(output_ph,1))\n",
    "accuracy = tf.reduce_mean(tf.cast(actual,tf.float32))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 919,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Initialise you variables\n",
    "\n",
    "batch_size = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 920,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration : 0, loss : 18829.76171875, accuracy : 0.1484375\n",
      "iteration : 1, loss : 329.94708251953125, accuracy : 0.1015625\n",
      "iteration : 2, loss : 14.337419509887695, accuracy : 0.109375\n",
      "iteration : 3, loss : 3.6372578144073486, accuracy : 0.1171875\n",
      "iteration : 4, loss : 4.120210647583008, accuracy : 0.1328125\n",
      "iteration : 5, loss : 2.7559189796447754, accuracy : 0.15625\n",
      "iteration : 6, loss : 2.341729164123535, accuracy : 0.140625\n",
      "iteration : 7, loss : 2.594339370727539, accuracy : 0.125\n",
      "iteration : 8, loss : 2.5110533237457275, accuracy : 0.1015625\n",
      "iteration : 9, loss : 2.3434762954711914, accuracy : 0.125\n",
      "iteration : 10, loss : 2.3762190341949463, accuracy : 0.109375\n",
      "iteration : 11, loss : 2.3954625129699707, accuracy : 0.1484375\n",
      "iteration : 12, loss : 2.4553308486938477, accuracy : 0.0625\n",
      "iteration : 13, loss : 2.3662376403808594, accuracy : 0.09375\n",
      "iteration : 14, loss : 2.336679697036743, accuracy : 0.1171875\n",
      "iteration : 15, loss : 2.300668239593506, accuracy : 0.09375\n",
      "iteration : 16, loss : 2.3260726928710938, accuracy : 0.1171875\n",
      "iteration : 17, loss : 2.355154514312744, accuracy : 0.0859375\n",
      "iteration : 18, loss : 2.3256711959838867, accuracy : 0.1171875\n",
      "iteration : 19, loss : 2.3484930992126465, accuracy : 0.1484375\n",
      "iteration : 20, loss : 2.3468010425567627, accuracy : 0.1015625\n",
      "iteration : 21, loss : 2.3142900466918945, accuracy : 0.1171875\n",
      "iteration : 22, loss : 2.3169689178466797, accuracy : 0.0859375\n",
      "iteration : 23, loss : 2.290034055709839, accuracy : 0.0703125\n",
      "iteration : 24, loss : 2.3332126140594482, accuracy : 0.125\n",
      "iteration : 25, loss : 2.319516181945801, accuracy : 0.1015625\n",
      "iteration : 26, loss : 2.300626754760742, accuracy : 0.125\n",
      "iteration : 27, loss : 2.304114818572998, accuracy : 0.1171875\n",
      "iteration : 28, loss : 2.2806692123413086, accuracy : 0.15625\n",
      "iteration : 29, loss : 2.282994031906128, accuracy : 0.125\n",
      "iteration : 30, loss : 2.3425159454345703, accuracy : 0.109375\n",
      "iteration : 31, loss : 2.3555572032928467, accuracy : 0.1015625\n",
      "iteration : 32, loss : 2.3576860427856445, accuracy : 0.0546875\n",
      "iteration : 33, loss : 2.325328826904297, accuracy : 0.0390625\n",
      "iteration : 34, loss : 2.296250104904175, accuracy : 0.1171875\n",
      "iteration : 35, loss : 2.277376174926758, accuracy : 0.15625\n",
      "iteration : 36, loss : 2.29460072517395, accuracy : 0.0859375\n",
      "iteration : 37, loss : 2.3465282917022705, accuracy : 0.0859375\n",
      "iteration : 38, loss : 2.2895960807800293, accuracy : 0.1328125\n",
      "iteration : 39, loss : 2.3441803455352783, accuracy : 0.0859375\n",
      "iteration : 40, loss : 2.317049503326416, accuracy : 0.1015625\n",
      "iteration : 41, loss : 2.302628993988037, accuracy : 0.125\n",
      "iteration : 42, loss : 2.3022282123565674, accuracy : 0.15625\n",
      "iteration : 43, loss : 2.3053369522094727, accuracy : 0.0625\n",
      "iteration : 44, loss : 2.2906956672668457, accuracy : 0.15625\n",
      "iteration : 45, loss : 2.278876304626465, accuracy : 0.1171875\n",
      "iteration : 46, loss : 2.344865083694458, accuracy : 0.0859375\n",
      "iteration : 47, loss : 2.329660177230835, accuracy : 0.0703125\n",
      "iteration : 48, loss : 2.3047733306884766, accuracy : 0.0859375\n",
      "iteration : 49, loss : 2.2999887466430664, accuracy : 0.1328125\n",
      "iteration : 50, loss : 2.3126142024993896, accuracy : 0.0546875\n",
      "iteration : 51, loss : 2.3166730403900146, accuracy : 0.078125\n",
      "iteration : 52, loss : 2.3071138858795166, accuracy : 0.140625\n",
      "iteration : 53, loss : 2.2908120155334473, accuracy : 0.1171875\n",
      "iteration : 54, loss : 2.3058085441589355, accuracy : 0.0859375\n",
      "iteration : 55, loss : 2.290404796600342, accuracy : 0.109375\n",
      "iteration : 56, loss : 2.3131046295166016, accuracy : 0.109375\n",
      "iteration : 57, loss : 2.296118974685669, accuracy : 0.1484375\n",
      "iteration : 58, loss : 2.302056312561035, accuracy : 0.1171875\n",
      "iteration : 59, loss : 2.310194492340088, accuracy : 0.09375\n",
      "iteration : 60, loss : 2.31554913520813, accuracy : 0.09375\n",
      "iteration : 61, loss : 2.317756175994873, accuracy : 0.1328125\n",
      "iteration : 62, loss : 2.3079237937927246, accuracy : 0.1171875\n",
      "iteration : 63, loss : 2.29093599319458, accuracy : 0.1484375\n",
      "iteration : 64, loss : 2.316256523132324, accuracy : 0.0859375\n",
      "iteration : 65, loss : 2.3100204467773438, accuracy : 0.1015625\n",
      "iteration : 66, loss : 2.2898364067077637, accuracy : 0.140625\n",
      "iteration : 67, loss : 2.3160176277160645, accuracy : 0.0703125\n",
      "iteration : 68, loss : 2.3163881301879883, accuracy : 0.0703125\n",
      "iteration : 69, loss : 2.3028016090393066, accuracy : 0.1015625\n",
      "iteration : 70, loss : 2.3146631717681885, accuracy : 0.09375\n",
      "iteration : 71, loss : 2.2901337146759033, accuracy : 0.1015625\n",
      "iteration : 72, loss : 2.286172866821289, accuracy : 0.109375\n",
      "iteration : 73, loss : 2.3133249282836914, accuracy : 0.109375\n",
      "iteration : 74, loss : 2.2989559173583984, accuracy : 0.09375\n",
      "iteration : 75, loss : 2.2972116470336914, accuracy : 0.140625\n",
      "iteration : 76, loss : 2.299358606338501, accuracy : 0.1171875\n",
      "iteration : 77, loss : 2.3133091926574707, accuracy : 0.09375\n",
      "iteration : 78, loss : 2.280853271484375, accuracy : 0.140625\n",
      "iteration : 79, loss : 2.305758476257324, accuracy : 0.0703125\n",
      "iteration : 80, loss : 2.3179783821105957, accuracy : 0.125\n",
      "iteration : 81, loss : 2.302855968475342, accuracy : 0.1015625\n",
      "iteration : 82, loss : 2.2904276847839355, accuracy : 0.140625\n",
      "iteration : 83, loss : 2.304971694946289, accuracy : 0.1015625\n",
      "iteration : 84, loss : 2.2941842079162598, accuracy : 0.1015625\n",
      "iteration : 85, loss : 2.2958178520202637, accuracy : 0.15625\n",
      "iteration : 86, loss : 2.298534393310547, accuracy : 0.1328125\n",
      "iteration : 87, loss : 2.3058056831359863, accuracy : 0.1015625\n",
      "iteration : 88, loss : 2.333110809326172, accuracy : 0.0390625\n",
      "iteration : 89, loss : 2.2957682609558105, accuracy : 0.0546875\n",
      "iteration : 90, loss : 2.3169147968292236, accuracy : 0.1171875\n",
      "iteration : 91, loss : 2.2983078956604004, accuracy : 0.1484375\n",
      "iteration : 92, loss : 2.3090627193450928, accuracy : 0.125\n",
      "iteration : 93, loss : 2.286802291870117, accuracy : 0.0859375\n",
      "iteration : 94, loss : 2.303274154663086, accuracy : 0.1171875\n",
      "iteration : 95, loss : 2.3066048622131348, accuracy : 0.1171875\n",
      "iteration : 96, loss : 2.290043354034424, accuracy : 0.1484375\n",
      "iteration : 97, loss : 2.305166244506836, accuracy : 0.125\n",
      "iteration : 98, loss : 2.3135175704956055, accuracy : 0.125\n",
      "iteration : 99, loss : 2.2953829765319824, accuracy : 0.125\n"
     ]
    }
   ],
   "source": [
    "#Iterate and check if loss reduces\n",
    "\n",
    "for i in range(100):\n",
    "    batch_input , batch_output = mnist.train.next_batch(batch_size)\n",
    "    _,mse = sess.run([opt,loss],feed_dict={input_ph: batch_input, output_ph: batch_output})\n",
    "    \n",
    "    \n",
    "#print loss w.r.t current iteration value    \n",
    "    \n",
    "    mse,acr = sess.run([loss,accuracy],feed_dict={input_ph:batch_input, output_ph:batch_output})\n",
    "    print(\"iteration : {}, loss : {}, accuracy : {}\".format(i,mse,acr))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
