{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 889,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' \\n   Size = 28x28\\n   Representation : 28x28 = \\n   Training set : 60,000\\n   Test set : 10,000       '"
      ]
     },
     "execution_count": 889,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "''' \n",
    "   Size = 28x28\n",
    "   Representation : 28x28 = \n",
    "   Training set : 60,000\n",
    "   Test set : 10,000       '''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 890,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting data/train-images-idx3-ubyte.gz\n",
      "Extracting data/train-labels-idx1-ubyte.gz\n",
      "Extracting data/t10k-images-idx3-ubyte.gz\n",
      "Extracting data/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "#imports\n",
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
    "import ssl\n",
    "ssl._create_default_https_context = ssl._create_unverified_context\n",
    "\n",
    "# Import MNIST data\n",
    "#http://yann.lecun.com/exdb/mnist/\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "mnist = input_data.read_data_sets(\"data/\", one_hot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 891,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Write function to reset session\n",
    "\n",
    "def tf_reset():\n",
    "    try:\n",
    "        sess.close()\n",
    "    except:\n",
    "        pass\n",
    "    tf.reset_default_graph()\n",
    "    return tf.Session()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 892,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Call the reset function\n",
    "\n",
    "sess = tf_reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 893,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define input_placeholder\n",
    "\n",
    "input_ph = tf.placeholder(dtype=tf.float32, shape=[None, 784]) #inputs 28X28=784"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 894,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define output_placeholder\n",
    "\n",
    "n_classes = 10\n",
    "output_ph = tf.placeholder(dtype=tf.float32, shape=[None, n_classes]) #MNIST classes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 895,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Store weigth and biases\n",
    "\n",
    "\n",
    "def get_weights_bias(n_classes): \n",
    "    w = {\n",
    "        \"w1\" : tf.get_variable('W0', shape=(5,5,1,32), initializer=tf.contrib.layers.xavier_initializer()), \n",
    "        \"w2\" : tf.get_variable('W1', shape=(5,5,32,64), initializer=tf.contrib.layers.xavier_initializer()),\n",
    "        'wd1': tf.get_variable('W3', shape=(7*7*64,1024), initializer=tf.contrib.layers.xavier_initializer()), \n",
    "        'out': tf.get_variable('W4', shape=(1024,10), initializer=tf.contrib.layers.xavier_initializer())\n",
    "    }\n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "    b = {\n",
    "        \"b1\" : tf.get_variable('B0', shape=(32), initializer=tf.contrib.layers.xavier_initializer()),\n",
    "        \"b2\" : tf.get_variable('B1', shape=(64), initializer=tf.contrib.layers.xavier_initializer()),\n",
    "        \"b4\" : tf.get_variable('B3', shape=(1024), initializer=tf.contrib.layers.xavier_initializer()),\n",
    "        \"b5\" : tf.get_variable('B4', shape=(10), initializer=tf.contrib.layers.xavier_initializer())\n",
    "    }\n",
    "\n",
    "    return w,b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 896,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Convolve, MaxPool, FullyConnect\n",
    "\n",
    "def conv2d(X,w,b,strides=1):\n",
    "    #building conv-layer\n",
    "    x = tf.nn.conv2d(X,w,strides=[1,strides,strides,1],padding=\"SAME\")\n",
    "    x = tf.nn.bias_add(x,b)\n",
    "    return tf.nn.relu(x)\n",
    "\n",
    "def maxpool2d(X, k=2): #k defines how much you want to reduce your matrix\n",
    "    return tf.nn.max_pool(X, ksize=[1, k, k, 1], strides=[1, k, k, 1],padding='SAME')\n",
    "\n",
    "def fully_connected(X,w,b): #reduce from 2d to 1d\n",
    "    fc = tf.reshape(X, [-1,w.get_shape().as_list()[0]])\n",
    "    fc = tf.matmul(fc, w)\n",
    "    fc = tf.add(fc,b)\n",
    "    return tf.nn.relu(fc)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 897,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Build a model\n",
    "\n",
    "\n",
    "def build_model(input_layer,w,b):\n",
    "\n",
    "    input_layer = tf.reshape(input_layer,[-1,28,28,1])\n",
    "\n",
    "    #conv-layer-1\n",
    "    conv1 = conv2d(input_layer,w['w1'],b['b1'])\n",
    "    conv1 = maxpool2d(conv1, k=2)\n",
    "    print(conv1.get_shape())\n",
    "\n",
    "    conv2 = conv2d(conv1,w['w2'],b['b2'])\n",
    "    conv2 = maxpool2d(conv2, k=2)\n",
    "    print(conv2.get_shape())\n",
    "\n",
    "    fc1 = fully_connected(conv2,w['wd1'],b['b4'])\n",
    "    print(fc1.get_shape())\n",
    "\n",
    "    out = tf.matmul(fc1,w['out'])\n",
    "\n",
    "    return tf.add(out,b['b5'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 898,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(?, 14, 14, 32)\n",
      "(?, 7, 7, 64)\n",
      "(?, 1024)\n"
     ]
    }
   ],
   "source": [
    "#Output_pred = build_model(input_layer,w,b)      \n",
    "\n",
    "w,b = get_weights_bias(10)                                                 \n",
    "output_pred = build_model(input_ph,w,b)          \n",
    "sess.run(tf.global_variables_initializer())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 899,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define Loss function\n",
    "\n",
    "loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=output_pred,labels=output_ph))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 900,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define Optimizer\n",
    "\n",
    "opt = tf.train.GradientDescentOptimizer(learning_rate = 0.1).minimize(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 901,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"Softmax:0\", shape=(?, 10), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "#Run session to compute loss\n",
    "\n",
    "pred = tf.nn.softmax(output_pred)\n",
    "sess.run(tf.global_variables_initializer())\n",
    "print (pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 902,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Try for accuracy\n",
    "\n",
    "actual = tf.equal(tf.argmax(pred,1),tf.argmax(output_ph,1))\n",
    "accuracy = tf.reduce_mean(tf.cast(actual,tf.float32))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 903,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Initialise you variables\n",
    "\n",
    "batch_size = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 904,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration : 0, loss : 2.2556118965148926, accuracy : 0.1484375\n",
      "iteration : 1, loss : 2.2280848026275635, accuracy : 0.1953125\n",
      "iteration : 2, loss : 2.21100115776062, accuracy : 0.1953125\n",
      "iteration : 3, loss : 2.1724252700805664, accuracy : 0.28125\n",
      "iteration : 4, loss : 2.1512255668640137, accuracy : 0.2734375\n",
      "iteration : 5, loss : 2.1792898178100586, accuracy : 0.3984375\n",
      "iteration : 6, loss : 2.1346547603607178, accuracy : 0.15625\n",
      "iteration : 7, loss : 2.109809637069702, accuracy : 0.5078125\n",
      "iteration : 8, loss : 2.0354928970336914, accuracy : 0.1875\n",
      "iteration : 9, loss : 2.0340209007263184, accuracy : 0.4765625\n",
      "iteration : 10, loss : 1.9782116413116455, accuracy : 0.34375\n",
      "iteration : 11, loss : 1.8901970386505127, accuracy : 0.4453125\n",
      "iteration : 12, loss : 1.7647019624710083, accuracy : 0.609375\n",
      "iteration : 13, loss : 1.7694833278656006, accuracy : 0.46875\n",
      "iteration : 14, loss : 1.6917874813079834, accuracy : 0.59375\n",
      "iteration : 15, loss : 1.5935510396957397, accuracy : 0.5625\n",
      "iteration : 16, loss : 1.7938127517700195, accuracy : 0.390625\n",
      "iteration : 17, loss : 1.8992949724197388, accuracy : 0.4296875\n",
      "iteration : 18, loss : 1.5837866067886353, accuracy : 0.4375\n",
      "iteration : 19, loss : 1.2718085050582886, accuracy : 0.71875\n",
      "iteration : 20, loss : 1.0597116947174072, accuracy : 0.6640625\n",
      "iteration : 21, loss : 1.2745895385742188, accuracy : 0.5546875\n",
      "iteration : 22, loss : 1.8821431398391724, accuracy : 0.578125\n",
      "iteration : 23, loss : 1.731799602508545, accuracy : 0.53125\n",
      "iteration : 24, loss : 1.2850699424743652, accuracy : 0.6015625\n",
      "iteration : 25, loss : 0.7870325446128845, accuracy : 0.8203125\n",
      "iteration : 26, loss : 0.9534318447113037, accuracy : 0.71875\n",
      "iteration : 27, loss : 0.9624695181846619, accuracy : 0.6796875\n",
      "iteration : 28, loss : 0.7532194256782532, accuracy : 0.8125\n",
      "iteration : 29, loss : 0.7630181312561035, accuracy : 0.7265625\n",
      "iteration : 30, loss : 1.2113916873931885, accuracy : 0.640625\n",
      "iteration : 31, loss : 1.188664436340332, accuracy : 0.71875\n",
      "iteration : 32, loss : 0.8117448091506958, accuracy : 0.8359375\n",
      "iteration : 33, loss : 0.696574330329895, accuracy : 0.828125\n",
      "iteration : 34, loss : 0.5388945937156677, accuracy : 0.828125\n",
      "iteration : 35, loss : 0.567590057849884, accuracy : 0.8203125\n",
      "iteration : 36, loss : 0.7733172178268433, accuracy : 0.7578125\n",
      "iteration : 37, loss : 0.6561601161956787, accuracy : 0.7890625\n",
      "iteration : 38, loss : 0.45978400111198425, accuracy : 0.84375\n",
      "iteration : 39, loss : 0.6041622161865234, accuracy : 0.7890625\n",
      "iteration : 40, loss : 0.5234229564666748, accuracy : 0.8359375\n",
      "iteration : 41, loss : 0.42249795794487, accuracy : 0.8671875\n",
      "iteration : 42, loss : 0.297673761844635, accuracy : 0.9296875\n",
      "iteration : 43, loss : 0.41293397545814514, accuracy : 0.8671875\n",
      "iteration : 44, loss : 0.573728621006012, accuracy : 0.8359375\n",
      "iteration : 45, loss : 0.5446244478225708, accuracy : 0.8359375\n",
      "iteration : 46, loss : 0.2996646761894226, accuracy : 0.921875\n",
      "iteration : 47, loss : 0.3388535976409912, accuracy : 0.8984375\n",
      "iteration : 48, loss : 0.3083156943321228, accuracy : 0.9140625\n",
      "iteration : 49, loss : 0.3680160641670227, accuracy : 0.8515625\n",
      "iteration : 50, loss : 0.49232879281044006, accuracy : 0.8515625\n",
      "iteration : 51, loss : 0.47877413034439087, accuracy : 0.8203125\n",
      "iteration : 52, loss : 0.4331708550453186, accuracy : 0.859375\n",
      "iteration : 53, loss : 0.3749493956565857, accuracy : 0.8984375\n",
      "iteration : 54, loss : 0.2399521917104721, accuracy : 0.9453125\n",
      "iteration : 55, loss : 0.3679516911506653, accuracy : 0.859375\n",
      "iteration : 56, loss : 0.4706137776374817, accuracy : 0.828125\n",
      "iteration : 57, loss : 0.42774254083633423, accuracy : 0.8828125\n",
      "iteration : 58, loss : 0.27996882796287537, accuracy : 0.9453125\n",
      "iteration : 59, loss : 0.20725972950458527, accuracy : 0.9453125\n",
      "iteration : 60, loss : 0.3033747673034668, accuracy : 0.9453125\n",
      "iteration : 61, loss : 0.3956746459007263, accuracy : 0.859375\n",
      "iteration : 62, loss : 0.4424000382423401, accuracy : 0.8359375\n",
      "iteration : 63, loss : 0.28808435797691345, accuracy : 0.921875\n",
      "iteration : 64, loss : 0.16267545521259308, accuracy : 0.96875\n",
      "iteration : 65, loss : 0.25107401609420776, accuracy : 0.9375\n",
      "iteration : 66, loss : 0.19753322005271912, accuracy : 0.953125\n",
      "iteration : 67, loss : 0.34052199125289917, accuracy : 0.8984375\n",
      "iteration : 68, loss : 0.27066513895988464, accuracy : 0.921875\n",
      "iteration : 69, loss : 0.22455096244812012, accuracy : 0.9375\n",
      "iteration : 70, loss : 0.2765178382396698, accuracy : 0.9375\n",
      "iteration : 71, loss : 0.26967084407806396, accuracy : 0.9296875\n",
      "iteration : 72, loss : 0.18392404913902283, accuracy : 0.9453125\n",
      "iteration : 73, loss : 0.33152472972869873, accuracy : 0.90625\n",
      "iteration : 74, loss : 0.21845880150794983, accuracy : 0.953125\n",
      "iteration : 75, loss : 0.18309377133846283, accuracy : 0.9609375\n",
      "iteration : 76, loss : 0.2296808362007141, accuracy : 0.9140625\n",
      "iteration : 77, loss : 0.22836284339427948, accuracy : 0.9375\n",
      "iteration : 78, loss : 0.21883374452590942, accuracy : 0.9296875\n",
      "iteration : 79, loss : 0.22086988389492035, accuracy : 0.921875\n",
      "iteration : 80, loss : 0.18826961517333984, accuracy : 0.953125\n",
      "iteration : 81, loss : 0.2301851212978363, accuracy : 0.90625\n",
      "iteration : 82, loss : 0.14079345762729645, accuracy : 0.9609375\n",
      "iteration : 83, loss : 0.20205068588256836, accuracy : 0.9453125\n",
      "iteration : 84, loss : 0.32233351469039917, accuracy : 0.8671875\n",
      "iteration : 85, loss : 0.14488378167152405, accuracy : 0.96875\n",
      "iteration : 86, loss : 0.29460614919662476, accuracy : 0.8828125\n",
      "iteration : 87, loss : 0.31275832653045654, accuracy : 0.890625\n",
      "iteration : 88, loss : 0.2596127390861511, accuracy : 0.9453125\n",
      "iteration : 89, loss : 0.13088646531105042, accuracy : 0.9765625\n",
      "iteration : 90, loss : 0.09945103526115417, accuracy : 0.9765625\n",
      "iteration : 91, loss : 0.17073950171470642, accuracy : 0.96875\n",
      "iteration : 92, loss : 0.19931066036224365, accuracy : 0.9453125\n",
      "iteration : 93, loss : 0.16821275651454926, accuracy : 0.9375\n",
      "iteration : 94, loss : 0.08747634291648865, accuracy : 0.9765625\n",
      "iteration : 95, loss : 0.20878735184669495, accuracy : 0.921875\n",
      "iteration : 96, loss : 0.21796205639839172, accuracy : 0.9296875\n",
      "iteration : 97, loss : 0.1558220386505127, accuracy : 0.953125\n",
      "iteration : 98, loss : 0.09196742624044418, accuracy : 1.0\n",
      "iteration : 99, loss : 0.18534904718399048, accuracy : 0.9453125\n"
     ]
    }
   ],
   "source": [
    "#Iterate and check if loss reduces\n",
    "\n",
    "for i in range(100):\n",
    "    batch_input , batch_output = mnist.train.next_batch(batch_size)\n",
    "    _,mse = sess.run([opt,loss],feed_dict={input_ph: batch_input, output_ph: batch_output})\n",
    "    \n",
    "    \n",
    "#print loss w.r.t current iteration value    \n",
    "    \n",
    "    mse,acr = sess.run([loss,accuracy],feed_dict={input_ph:batch_input, output_ph:batch_output})\n",
    "    print(\"iteration : {}, loss : {}, accuracy : {}\".format(i,mse,acr))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
